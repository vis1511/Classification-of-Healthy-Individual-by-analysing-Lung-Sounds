{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of healthy and Non healthy individual on the basis of their lung sounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`By VISHAL KUMAR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score,accuracy_score,roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read test and train datasets formed after preprocessing\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>split</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>is_healthy</th>\n",
       "      <th>disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>103_2b2_Ar_mc_LittC2SE</td>\n",
       "      <td>train</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>Asthma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>105_1b1_Tc_sc_Meditron</td>\n",
       "      <td>train</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.558299</td>\n",
       "      <td>0</td>\n",
       "      <td>URTI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>106_2b1_Pl_mc_LittC2SE</td>\n",
       "      <td>train</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>COPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>106_2b1_Pr_mc_LittC2SE</td>\n",
       "      <td>train</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>COPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>107_2b3_Al_mc_AKGC417L</td>\n",
       "      <td>train</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.700000</td>\n",
       "      <td>0</td>\n",
       "      <td>COPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>906</td>\n",
       "      <td>222_1b1_Lr_sc_Meditron</td>\n",
       "      <td>train</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>COPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>907</td>\n",
       "      <td>222_1b1_Pr_sc_Meditron</td>\n",
       "      <td>train</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>COPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>917</td>\n",
       "      <td>226_1b1_Al_sc_Meditron</td>\n",
       "      <td>train</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.741352</td>\n",
       "      <td>0</td>\n",
       "      <td>Pneumoni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>918</td>\n",
       "      <td>226_1b1_Ll_sc_Meditron</td>\n",
       "      <td>train</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.741352</td>\n",
       "      <td>0</td>\n",
       "      <td>Pneumoni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>919</td>\n",
       "      <td>226_1b1_Pl_sc_Meditron</td>\n",
       "      <td>train</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.741352</td>\n",
       "      <td>0</td>\n",
       "      <td>Pneumoni</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>539 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                filename  split   Age  Gender        BMI  \\\n",
       "0             3  103_2b2_Ar_mc_LittC2SE  train  70.0       0  33.000000   \n",
       "1            10  105_1b1_Tc_sc_Meditron  train   7.0       0  17.558299   \n",
       "2            11  106_2b1_Pl_mc_LittC2SE  train  73.0       0  21.000000   \n",
       "3            12  106_2b1_Pr_mc_LittC2SE  train  73.0       0  21.000000   \n",
       "4            13  107_2b3_Al_mc_AKGC417L  train  75.0       0  33.700000   \n",
       "..          ...                     ...    ...   ...     ...        ...   \n",
       "534         906  222_1b1_Lr_sc_Meditron  train  60.0       1  22.000000   \n",
       "535         907  222_1b1_Pr_sc_Meditron  train  60.0       1  22.000000   \n",
       "536         917  226_1b1_Al_sc_Meditron  train   4.0       1  15.741352   \n",
       "537         918  226_1b1_Ll_sc_Meditron  train   4.0       1  15.741352   \n",
       "538         919  226_1b1_Pl_sc_Meditron  train   4.0       1  15.741352   \n",
       "\n",
       "     is_healthy   disease  \n",
       "0             0    Asthma  \n",
       "1             0      URTI  \n",
       "2             0      COPD  \n",
       "3             0      COPD  \n",
       "4             0      COPD  \n",
       "..          ...       ...  \n",
       "534           0      COPD  \n",
       "535           0      COPD  \n",
       "536           0  Pneumoni  \n",
       "537           0  Pneumoni  \n",
       "538           0  Pneumoni  \n",
       "\n",
       "[539 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    521\n",
       "1     18\n",
       "Name: is_healthy, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['is_healthy'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COPD              444\n",
       "Pneumonia          34\n",
       "Healthy            18\n",
       "URTI               16\n",
       "Bronchiectasis     14\n",
       "Bronchiolitis       7\n",
       "Pneumoni            3\n",
       "LRTI                2\n",
       "Asthma              1\n",
       "Name: disease, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['disease'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the imbalance of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we will load data \n",
    "We use Librosa to extract MFCC audio features to be used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.getcwd()\n",
    "\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "for i in range(len(train_data)):\n",
    "    # if train_data[\"is_healthy\"][i]!=1:\n",
    "    file = dir+\"/\"+train_data[\"filename\"][i]+\".wav\"\n",
    "    data_x, sampling_rate = librosa.load(file)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
    "    X_train.append(mfccs)\n",
    "    y_train.append(train_data[\"is_healthy\"][i])\n",
    "\n",
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=[]\n",
    "y_test=[]\n",
    "for i in range(len(test_data)):\n",
    "    file = dir+\"/\"+test_data[\"filename\"][i]+\".wav\"\n",
    "    data_x, sampling_rate = librosa.load(file)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
    "    X_test.append(mfccs)\n",
    "    y_test.append(train_data[\"is_healthy\"][i])\n",
    "\n",
    "X_test=np.array(X_test)\n",
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will fir some classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First is an ensemble technique Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the data before training\n",
    "X_shuffled,y_shuffled = shuffle(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Training Data\n",
      "Accuracy:  0.9424860853432282\n",
      "Precision:  0.3673469387755102\n",
      "Recall:  1.0\n",
      "F1 score:  0.5373134328358209\n",
      "AUC-ROC score:  0.9702495201535509\n",
      "For Test data\n",
      "Accuracy:  0.905511811023622\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1 score:  0.0\n",
      "AUC-ROC score:  0.4713114754098361\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=15,max_depth=2, random_state=0, bootstrap=True, oob_score=True,class_weight=\"balanced\")\n",
    "#using balanced class weights since it is an imbalanced dataset\n",
    "clf.fit(X_shuffled,y_shuffled)\n",
    "\n",
    "print(\"For Training Data\")\n",
    "print(\"Accuracy: \",accuracy_score(y_shuffled,clf.predict(X_shuffled)))\n",
    "print(\"Precision: \",precision_score(y_shuffled,clf.predict(X_shuffled)))\n",
    "print(\"Recall: \",recall_score(y_shuffled,clf.predict(X_shuffled)))\n",
    "print(\"F1 score: \",f1_score(y_shuffled,clf.predict(X_shuffled)))\n",
    "print(\"AUC-ROC score: \",roc_auc_score(y_shuffled,clf.predict(X_shuffled)))\n",
    "\n",
    "print(\"For Test data\")\n",
    "print(\"Accuracy: \",accuracy_score(y_test,clf.predict(X_test)))\n",
    "print(\"Precision: \",precision_score(y_test,clf.predict(X_test)))\n",
    "print(\"Recall: \",recall_score(y_test,clf.predict(X_test)))\n",
    "print(\"F1 score: \",f1_score(y_test,clf.predict(X_test)))\n",
    "print(\"AUC-ROC score: \",roc_auc_score(y_test,clf.predict(X_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting poor performance on test data \n",
    "Let's try some data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to augment data\n",
    "def add_noise(data,x):\n",
    "    noise = np.random.randn(len(data))\n",
    "    data_noise = data + x * noise\n",
    "    return data_noise\n",
    "\n",
    "def shift(data,x):\n",
    "    return np.roll(data, x)\n",
    "\n",
    "def stretch(data, rate):\n",
    "    data = librosa.effects.time_stretch(data, rate)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=[]\n",
    "y_train=[]\n",
    "for i in range(len(train_data)):\n",
    "    if train_data[\"is_healthy\"][i]!=1:\n",
    "        file = dir+\"/\"+train_data[\"filename\"][i]+\".wav\"\n",
    "        data_x, sampling_rate = librosa.load(file)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
    "        X_train.append(mfccs)\n",
    "        y_train.append(train_data[\"is_healthy\"][i])\n",
    "        \n",
    "    else:\n",
    "        file = dir+\"/\"+train_data[\"filename\"][i]+\".wav\"\n",
    "        data_x, sampling_rate = librosa.load(file)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
    "\n",
    "        X_train.append(mfccs)\n",
    "        y_train.append(train_data[\"is_healthy\"][i])\n",
    "\n",
    "\n",
    "        data_noise = add_noise(data_x,0.01)\n",
    "        mfccs_noise = np.mean(librosa.feature.mfcc(y=data_noise, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
    "        X_train.append(mfccs_noise)\n",
    "        y_train.append(train_data[\"is_healthy\"][i])\n",
    "\n",
    "\n",
    "        data_shift = shift(data_x,3200)\n",
    "        mfccs_shift = np.mean(librosa.feature.mfcc(y=data_shift, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
    "        X_train.append(mfccs_shift)\n",
    "        y_train.append(train_data[\"is_healthy\"][i])\n",
    "\n",
    "        data_stretch = stretch(data_x,1.2)\n",
    "        mfccs_stretch = np.mean(librosa.feature.mfcc(y=data_stretch, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
    "        X_train.append(mfccs_stretch)\n",
    "        y_train.append(train_data[\"is_healthy\"][i])\n",
    "\n",
    "        data_stretch_2 = stretch(data_x,0.8)\n",
    "        mfccs_stretch_2 = np.mean(librosa.feature.mfcc(y=data_stretch_2, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
    "        X_train.append(mfccs_stretch_2)\n",
    "        y_train.append(train_data[\"is_healthy\"][i])\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Training Data\n",
      "Accuracy:  0.911620294599018\n",
      "Precision:  0.6285714285714286\n",
      "Recall:  0.9777777777777777\n",
      "F1 score:  0.7652173913043477\n",
      "AUC-ROC score:  0.9389848581787161\n",
      "For Test data\n",
      "Accuracy:  0.8713910761154856\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1 score:  0.0\n",
      "AUC-ROC score:  0.453551912568306\n"
     ]
    }
   ],
   "source": [
    "X_shuffled_1,y_shuffled_1 = shuffle(X_train,y_train)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=20,max_depth=2, random_state=0, bootstrap=True, oob_score=True,class_weight=\"balanced\")\n",
    "clf.fit(X_shuffled_1,y_shuffled_1)\n",
    "\n",
    "print(\"For Training Data\")\n",
    "print(\"Accuracy: \",accuracy_score(y_shuffled_1,clf.predict(X_shuffled_1)))\n",
    "print(\"Precision: \",precision_score(y_shuffled_1,clf.predict(X_shuffled_1)))\n",
    "print(\"Recall: \",recall_score(y_shuffled_1,clf.predict(X_shuffled_1)))\n",
    "print(\"F1 score: \",f1_score(y_shuffled_1,clf.predict(X_shuffled_1)))\n",
    "print(\"AUC-ROC score: \",roc_auc_score(y_shuffled_1,clf.predict(X_shuffled_1)))\n",
    "\n",
    "print(\"For Test data\")\n",
    "print(\"Accuracy: \",accuracy_score(y_test,clf.predict(X_test)))\n",
    "print(\"Precision: \",precision_score(y_test,clf.predict(X_test)))\n",
    "print(\"Recall: \",recall_score(y_test,clf.predict(X_test)))\n",
    "print(\"F1 score: \",f1_score(y_test,clf.predict(X_test)))\n",
    "print(\"AUC-ROC score: \",roc_auc_score(y_test,clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try oversampling with this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    521\n",
       "1    521\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversample = SMOTE(random_state=0)\n",
    "X_train_over,y_train_over = oversample.fit_resample(X_train,y_train)\n",
    "\n",
    "Z=pd.DataFrame(y_train_over,columns=[\"target\"])\n",
    "Z[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Training Data\n",
      "Accuracy:  0.9414587332053743\n",
      "Precision:  0.9151624548736462\n",
      "Recall:  0.9731285988483686\n",
      "F1 score:  0.9432558139534883\n",
      "AUC-ROC score:  0.9414587332053743\n",
      "For Test data\n",
      "Accuracy:  0.847769028871391\n",
      "Precision:  0.022222222222222223\n",
      "Recall:  0.06666666666666667\n",
      "F1 score:  0.03333333333333333\n",
      "AUC-ROC score:  0.473224043715847\n"
     ]
    }
   ],
   "source": [
    "X_shuffled_over,y_shuffled_over= shuffle(X_train_over,y_train_over)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=20,max_depth=2, random_state=0, bootstrap=True, oob_score=True)\n",
    "clf.fit(X_shuffled_over,y_shuffled_over)\n",
    "\n",
    "print(\"For Training Data\")\n",
    "print(\"Accuracy: \",accuracy_score(y_shuffled_over,clf.predict(X_shuffled_over)))\n",
    "print(\"Precision: \",precision_score(y_shuffled_over,clf.predict(X_shuffled_over)))\n",
    "print(\"Recall: \",recall_score(y_shuffled_over,clf.predict(X_shuffled_over)))\n",
    "print(\"F1 score: \",f1_score(y_shuffled_over,clf.predict(X_shuffled_over)))\n",
    "print(\"AUC-ROC score: \",roc_auc_score(y_shuffled_over,clf.predict(X_shuffled_over)))\n",
    "\n",
    "print(\"For Test data\")\n",
    "print(\"Accuracy: \",accuracy_score(y_test,clf.predict(X_test)))\n",
    "print(\"Precision: \",precision_score(y_test,clf.predict(X_test)))\n",
    "print(\"Recall: \",recall_score(y_test,clf.predict(X_test)))\n",
    "print(\"F1 score: \",f1_score(y_test,clf.predict(X_test)))\n",
    "print(\"AUC-ROC score: \",roc_auc_score(y_test,clf.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still the training metrics are getting better and better but the test ones aren't"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying by adding demographic features to the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_demographic=[]\n",
    "y_train_demographic=[]\n",
    "for i in range(len(train_data)):\n",
    "    if train_data[\"is_healthy\"][i]!=1:\n",
    "        file = dir+\"/\"+train_data[\"filename\"][i]+\".wav\"\n",
    "        data_x, sampling_rate = librosa.load(file)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
    "        mfccs = np.append(mfccs,np.array([train_data[\"Age\"][i],train_data[\"Gender\"][i],train_data[\"BMI\"][i]]))\n",
    "        X_train_demographic.append(mfccs)\n",
    "        y_train_demographic.append(train_data[\"is_healthy\"][i])\n",
    "        \n",
    "    else:\n",
    "        file = dir+\"/\"+train_data[\"filename\"][i]+\".wav\"\n",
    "        data_x, sampling_rate = librosa.load(file)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
    "        mfccs = np.append(mfccs,np.array([train_data[\"Age\"][i],train_data[\"Gender\"][i],train_data[\"BMI\"][i]]))\n",
    "\n",
    "        X_train_demographic.append(mfccs)\n",
    "        y_train_demographic.append(train_data[\"is_healthy\"][i])\n",
    "\n",
    "\n",
    "        data_noise = add_noise(data_x,0.01)\n",
    "        mfccs_noise = np.mean(librosa.feature.mfcc(y=data_noise, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
    "        mfccs_noise = np.append(mfccs_noise,np.array([train_data[\"Age\"][i],train_data[\"Gender\"][i],train_data[\"BMI\"][i]]))\n",
    "        X_train_demographic.append(mfccs_noise)\n",
    "        y_train_demographic.append(train_data[\"is_healthy\"][i])\n",
    "\n",
    "\n",
    "        data_shift = shift(data_x,3200)\n",
    "        mfccs_shift = np.mean(librosa.feature.mfcc(y=data_shift, sr=sampling_rate, n_mfcc=40).T,axis=0)\n",
    "        mfccs_shift = np.append(mfccs_shift,np.array([train_data[\"Age\"][i],train_data[\"Gender\"][i],train_data[\"BMI\"][i]])) \n",
    "        X_train_demographic.append(mfccs_shift)\n",
    "        y_train_demographic.append(train_data[\"is_healthy\"][i])\n",
    "\n",
    "        data_stretch = stretch(data_x,1.2)\n",
    "        mfccs_stretch = np.mean(librosa.feature.mfcc(y=data_stretch, sr=sampling_rate, n_mfcc=40).T,axis=0)\n",
    "        mfccs_stretch = np.append(mfccs_stretch,np.array([train_data[\"Age\"][i],train_data[\"Gender\"][i],train_data[\"BMI\"][i]])) \n",
    "        X_train_demographic.append(mfccs_stretch)\n",
    "        y_train_demographic.append(train_data[\"is_healthy\"][i])\n",
    "\n",
    "        data_stretch_2 = stretch(data_x,0.8)\n",
    "        mfccs_stretch_2 = np.mean(librosa.feature.mfcc(y=data_stretch_2, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
    "        mfccs_stretch_2 = np.append(mfccs_stretch_2,np.array([train_data[\"Age\"][i],train_data[\"Gender\"][i],train_data[\"BMI\"][i]]))\n",
    "        X_train_demographic.append(mfccs_stretch_2)\n",
    "        y_train_demographic.append(train_data[\"is_healthy\"][i])\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "X_train_demographic=np.array(X_train_demographic)\n",
    "y_train_demographic=np.array(y_train_demographic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_demographic=[]\n",
    "y_test_demographic=[]\n",
    "for i in range(len(test_data)):\n",
    "    file = dir+\"/\"+test_data[\"filename\"][i]+\".wav\"\n",
    "    data_x, sampling_rate = librosa.load(file)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
    "    mfccs = np.append(mfccs,np.array([train_data[\"Age\"][i],train_data[\"Gender\"][i],train_data[\"BMI\"][i]]))\n",
    "    X_test_demographic.append(mfccs)\n",
    "    y_test_demographic.append(train_data[\"is_healthy\"][i])\n",
    "\n",
    "X_test_demographic=np.array(X_test_demographic)\n",
    "y_test_demographic=np.array(y_test_demographic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(611, 43)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_demographic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Training Data\n",
      "Accuracy:  0.9623567921440261\n",
      "Precision:  0.8018018018018018\n",
      "Recall:  0.9888888888888889\n",
      "F1 score:  0.8855721393034827\n",
      "AUC-ROC score:  0.9733312006824484\n",
      "For Test data\n",
      "Accuracy:  0.9291338582677166\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1 score:  0.0\n",
      "AUC-ROC score:  0.48360655737704916\n"
     ]
    }
   ],
   "source": [
    "X_shuffled_demographic,y_shuffled_demographic= shuffle(X_train_demographic,y_train_demographic)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=20,max_depth=2, random_state=0, bootstrap=True, oob_score=True,class_weight=\"balanced\")\n",
    "clf.fit(X_shuffled_demographic,y_shuffled_demographic)\n",
    "\n",
    "print(\"For Training Data\")\n",
    "print(\"Accuracy: \",accuracy_score(y_shuffled_demographic,clf.predict(X_shuffled_demographic)))\n",
    "print(\"Precision: \",precision_score(y_shuffled_demographic,clf.predict(X_shuffled_demographic)))\n",
    "print(\"Recall: \",recall_score(y_shuffled_demographic,clf.predict(X_shuffled_demographic)))\n",
    "print(\"F1 score: \",f1_score(y_shuffled_demographic,clf.predict(X_shuffled_demographic)))\n",
    "print(\"AUC-ROC score: \",roc_auc_score(y_shuffled_demographic,clf.predict(X_shuffled_demographic)))\n",
    "\n",
    "print(\"For Test data\")\n",
    "print(\"Accuracy: \",accuracy_score(y_test_demographic,clf.predict(X_test_demographic)))\n",
    "print(\"Precision: \",precision_score(y_test_demographic,clf.predict(X_test_demographic)))\n",
    "print(\"Recall: \",recall_score(y_test_demographic,clf.predict(X_test_demographic)))\n",
    "print(\"F1 score: \",f1_score(y_test_demographic,clf.predict(X_test_demographic)))\n",
    "print(\"AUC-ROC score: \",roc_auc_score(y_test_demographic,clf.predict(X_test_demographic)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing much happened again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summurize its all happening as we have a very few examples of Healthy labels and the classifier is memorizing them from training so as we get such a hig recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way we can try is to try a clustering approach to detect outliers. We do this using IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IsolationForest(contamination=0.04)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = IsolationForest(contamination=0.04)\n",
    "model.fit(X_shuffled[y_shuffled==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.931758530183727\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1 score:  0.0\n",
      "AUC-ROC score:  0.4849726775956284\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(X_test)\n",
    "res[res==1]=0\n",
    "res[res==-1]=1\n",
    "print(\"Accuracy: \",accuracy_score(y_test,res))\n",
    "print(\"Precision: \",precision_score(y_test,res))\n",
    "print(\"Recall: \",recall_score(y_test,res))\n",
    "print(\"F1 score: \",f1_score(y_test,res))\n",
    "print(\"AUC-ROC score: \",roc_auc_score(y_test,res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try neural networks approach for this problem but due to the less data available it wont be learning much anyway.\n",
    "But lets try..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset loader \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,data,label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        data = torch.tensor(self.data[idx],dtype=torch.float32)\n",
    "        label = torch.tensor(self.label[idx],dtype=torch.float32)\n",
    "        return data,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_train_over = np.array(X_shuffled_demographic).reshape(X_shuffled_demographic.shape[0],1,X_shuffled_demographic.shape[1])\n",
    "GRU_test = np.array(X_test_demographic).reshape(X_test_demographic.shape[0],1,X_test_demographic.shape[1])\n",
    "print(GRU_train_over.shape,GRU_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataset(X_shuffled_demographic,y_shuffled_demographic)\n",
    "test_set = CustomDataset(X_test_demographic,y_test_demographic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size=8)\n",
    "valid_loader = torch.utils.data.DataLoader(test_set,batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(611, 43)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_shuffled_demographic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(43, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8,1),\n",
    "            nn.Sigmoid()\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=43, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=32, out_features=8, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=8, out_features=1, bias=True)\n",
      "    (9): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([[0.3943]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1,43, device=device)\n",
    "logits = model(X)\n",
    "# pred_probab = nn.Softmax(dim=1)(logits)\n",
    "# y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {logits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "valid_max_acc = 0\n",
    "train_acc = torch.zeros(n_epochs)\n",
    "train_loss = torch.zeros(n_epochs)\n",
    "valid_acc = torch.zeros(n_epochs)\n",
    "valid_loss = torch.zeros(n_epochs)\n",
    "model = NeuralNetwork()\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.005,momentum=0.9)\n",
    "criterion = torch.nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 237.69it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 601.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining acc: 0.816704 \tTraining Loss: 0.173545\n",
      "Epoch: 1 \tValidation acc: 0.923828 \tValidation Loss: 0.065926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 270.90it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 659.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tTraining acc: 0.733676 \tTraining Loss: 0.172156\n",
      "Epoch: 2 \tValidation acc: 0.941406 \tValidation Loss: 0.139932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 318.76it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 982.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining acc: 0.738140 \tTraining Loss: 0.134983\n",
      "Epoch: 3 \tValidation acc: 0.941406 \tValidation Loss: 0.139292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 311.32it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 962.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tTraining acc: 0.649080 \tTraining Loss: 0.314373\n",
      "Epoch: 4 \tValidation acc: 0.960938 \tValidation Loss: 0.105586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 312.17it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 863.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tTraining acc: 0.631358 \tTraining Loss: 0.285744\n",
      "Epoch: 5 \tValidation acc: 0.041667 \tValidation Loss: 0.720023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 328.95it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 547.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \tTraining acc: 0.821970 \tTraining Loss: 0.559102\n",
      "Epoch: 6 \tValidation acc: 0.960938 \tValidation Loss: 0.372573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 324.40it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 925.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \tTraining acc: 0.851190 \tTraining Loss: 0.447001\n",
      "Epoch: 7 \tValidation acc: 0.960938 \tValidation Loss: 0.287880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 316.42it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 925.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 \tTraining acc: 0.851190 \tTraining Loss: 0.427295\n",
      "Epoch: 8 \tValidation acc: 0.960938 \tValidation Loss: 0.257684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 312.43it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 943.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 \tTraining acc: 0.851190 \tTraining Loss: 0.422601\n",
      "Epoch: 9 \tValidation acc: 0.960938 \tValidation Loss: 0.243995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 310.07it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 962.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \tTraining acc: 0.851190 \tTraining Loss: 0.421330\n",
      "Epoch: 10 \tValidation acc: 0.960938 \tValidation Loss: 0.237003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 308.63it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 1024.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 \tTraining acc: 0.851190 \tTraining Loss: 0.420983\n",
      "Epoch: 11 \tValidation acc: 0.960938 \tValidation Loss: 0.233194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 333.04it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 982.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 \tTraining acc: 0.851190 \tTraining Loss: 0.420900\n",
      "Epoch: 12 \tValidation acc: 0.960938 \tValidation Loss: 0.231043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 320.36it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 962.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 \tTraining acc: 0.851190 \tTraining Loss: 0.420891\n",
      "Epoch: 13 \tValidation acc: 0.960938 \tValidation Loss: 0.229803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 316.44it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 982.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 \tTraining acc: 0.851190 \tTraining Loss: 0.420898\n",
      "Epoch: 14 \tValidation acc: 0.960938 \tValidation Loss: 0.229079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 316.37it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 930.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 \tTraining acc: 0.851190 \tTraining Loss: 0.420907\n",
      "Epoch: 15 \tValidation acc: 0.960938 \tValidation Loss: 0.228654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 311.13it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 981.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 \tTraining acc: 0.851190 \tTraining Loss: 0.420914\n",
      "Epoch: 16 \tValidation acc: 0.960938 \tValidation Loss: 0.228403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 300.41it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 981.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 \tTraining acc: 0.851190 \tTraining Loss: 0.420919\n",
      "Epoch: 17 \tValidation acc: 0.960938 \tValidation Loss: 0.228255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 315.13it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 923.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 \tTraining acc: 0.851190 \tTraining Loss: 0.420922\n",
      "Epoch: 18 \tValidation acc: 0.960938 \tValidation Loss: 0.228167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 320.40it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 943.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 \tTraining acc: 0.851190 \tTraining Loss: 0.420924\n",
      "Epoch: 19 \tValidation acc: 0.960938 \tValidation Loss: 0.228114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 303.97it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 1002.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 \tTraining acc: 0.851190 \tTraining Loss: 0.420925\n",
      "Epoch: 20 \tValidation acc: 0.960938 \tValidation Loss: 0.228083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 321.69it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 962.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 \tTraining acc: 0.851190 \tTraining Loss: 0.420925\n",
      "Epoch: 21 \tValidation acc: 0.960938 \tValidation Loss: 0.228065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 331.88it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 925.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 22 \tValidation acc: 0.960938 \tValidation Loss: 0.228054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 324.40it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 908.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 23 \tValidation acc: 0.960938 \tValidation Loss: 0.228048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 324.39it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 962.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 24 \tValidation acc: 0.960938 \tValidation Loss: 0.228044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 334.00it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 982.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 25 \tValidation acc: 0.960938 \tValidation Loss: 0.228041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 308.12it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 986.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 26 \tValidation acc: 0.960938 \tValidation Loss: 0.228040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 327.15it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 982.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 27 \tValidation acc: 0.960938 \tValidation Loss: 0.228039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 299.26it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 962.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 28 \tValidation acc: 0.960938 \tValidation Loss: 0.228039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 320.35it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 962.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 29 \tValidation acc: 0.960938 \tValidation Loss: 0.228039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 315.13it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 1002.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 30 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 317.72it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 943.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 31 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 315.13it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 1024.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 32 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 297.16it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 982.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 33 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 341.57it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 740.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 34 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 312.07it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 1000.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 35 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 323.20it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 983.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 36 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 336.83it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 925.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 37 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 316.21it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 940.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 38 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 305.43it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 549.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 39 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 312.67it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 1002.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 40 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 316.87it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 982.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 41 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 307.61it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 982.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 42 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 343.13it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 982.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 43 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 320.94it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 1002.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 44 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 305.46it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 998.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 45 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 334.54it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 648.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 46 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 309.56it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 982.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 47 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 312.22it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 962.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 48 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 332.79it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 1002.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 49 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:00<00:00, 347.09it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 982.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 \tTraining acc: 0.851190 \tTraining Loss: 0.420926\n",
      "Epoch: 50 \tValidation acc: 0.960938 \tValidation Loss: 0.228038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for e in range(0, n_epochs):\n",
    "    model.train()\n",
    "    for data, labels in tqdm(train_loader):\n",
    "        data, labels = data.to(device).float(), labels.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(data)\n",
    "        labels = labels.unsqueeze(0)\n",
    "        loss = criterion(logits.T, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss[e] += loss.item()\n",
    "\n",
    "        logits[logits>=0.5]=1\n",
    "        logits[logits<0.5]=0\n",
    "\n",
    "        # softmax = logits.softmax(dim=1)\n",
    "        # argmax = softmax.argmax(1)\n",
    "        equals = (logits == labels)\n",
    "\n",
    "        train_acc[e] += torch.mean(equals.type(torch.float)).detach().cpu()\n",
    "\n",
    "\n",
    "    train_loss[e] /= len(train_loader)\n",
    "    train_acc[e] /= len(train_loader)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data , labels in tqdm(valid_loader):\n",
    "            data, labels = data.to(device).float(), labels.to(device).float()\n",
    "            \n",
    "            logits = model(data)\n",
    "            labels = labels.unsqueeze(0)\n",
    "            loss = criterion(logits.T,labels)\n",
    "            \n",
    "            valid_loss[e] += loss.item()\n",
    "            logits[logits>=0.5]=1\n",
    "            logits[logits<0.5]=0\n",
    "            \n",
    "            # softmax = logits.softmax(dim=1)\n",
    "            # argmax = softmax.argmax(1)\n",
    "            equals = (logits == labels)\n",
    "            \n",
    "            valid_acc[e] += torch.mean(equals.type(torch.float)).detach().cpu()\n",
    "\n",
    "        valid_loss[e] /= len(valid_loader)\n",
    "        valid_acc[e] /= len(valid_loader)\n",
    "    print('Epoch: {} \\tTraining acc: {:.6f} \\tTraining Loss: {:.6f}'.format(\n",
    "        e+1, train_acc[e], train_loss[e]))\n",
    "    \n",
    "    print('Epoch: {} \\tValidation acc: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        e+1, valid_acc[e], valid_loss[e]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach one can try is to use RNN such as LSTM or GRU for training as they are quite suited for with sequence data.\n",
    "I dont have enough knowledge og them and hence need some more time to delve into that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "613fe122287fdb1a4092b1ec324ab5e18de9ec977608057b646301948d1df577"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
